---
title: "CLV Analysis: non-contractual settings"
author: "GEORGE KNOX"
date: "Computer Lab 7"
output: 
  html_document:
    highlight: haddock
    theme: journal
    number_sections: no
    toc: yes
    toc_depth: 2
    toc_float: yes
---
```{css, echo=FALSE}
    body .main-container {
      max-width: 1800px !important;
      width: 1800px !important;
    }
    body {
      max-width: 1800px !important;
    }
    ```
# Introduction

Customer lifetime value is the present value of the future profits associated with a particular customer.  In this section, we'll focus on non-contractual settings, in particular latent attrition or "Buy Til You Die" models.  All these models share a *common* feature: 

\center
**The customer's relationship with a firm has two phases: he is alive for some period of time, the becomes permanently inactive.** 
\center

The classic paper that launched this literature^[Schmittlein, D., Morrison, D., & Colombo, R. (1987). Counting Your Customers: Who Are They and What Will They Do Next? Management Science, 33(1), 1-24. Retrieved November 28, 2020, from http://www.jstor.org/stable/2631608] created the Pareto/NBD model.  A better place to start for us however is the Beta-geometric/Beta-binomial (BG/BB) model, because it is similar to the sBG model we covered in the contractual setting.

There is an excellent package in R maintained for a suite of these models, "BTYD", however it was recently removed from CRAN.  As a result we have to download an archived version of this package and some other ones. This may give you a few error messages that you need to install Rtools, but they can be ignored.

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
# dijfsadjfialsdfjsa;l
# uncomment when running first time

#install.packages("devtools")
#install.packages('hypergeo')
#install.packages('https://cran.r-project.org/src/contrib/Archive/BTYD/BTYD_2.4.tar.gz', repos = NULL, type = "source")

rm(list=ls())
options("scipen"=100, "digits"=3, width = 150)
library("BTYD")

```

# The BG/BB model

The best place to start off is the Beta-geometric/Beta-Binomial model (BG/BB), which is in discrete time. This is the same discrete time we used with the sBG (shifted Beta-geometric) model for contractual settings.  The assumptions of the model are given in the research paper^[Fader, Peter S., Bruce G.S. Hardie, and Jen
Shang. "Customer-Base Analysis in a Discrete-Time Noncontractual Setting." *Marketing Science*, **29(6)**,
pp. 1086-1108. 2010. INFORMS. [link](http://www.brucehardie.com/papers/020/)]:

1. A customer's relationship with the firm has two phases: he is alive (A) for some period of time, then becomes permanently inactive ("dies"; D).

2. While alive, a customer makes a purchase with probability $p$ in any given period:
$$
P(Y(t) = 1 \mid p, \textrm{alive at} \; t) = p, \quad 0 \leq p \leq 1
$$
This implies that a customer alive for $s$ periods makes a number of purchases according to a Binomial$(s, p)$ distribution. 

3. A “living” customer dies at the beginning of a transaction opportunity with probability \theta. (This
implies that the (unobserved) lifetime of a customer is characterized by a geometric distribution.^[It's not shifted because 0 is a valid outcome.])
$$
P( \textrm{alive at} \; t \mid \theta)= P(T>t \mid \theta) = S(t \mid \theta ) = (1-\theta)^t \quad 0 \leq \theta \leq 1
$$

4. Heterogeneity in $p$ follows a beta distribution with parameters $\alpha$ and $\beta$.
$$ f(p \; | \; \alpha,\beta) = \frac{p^{\alpha-1} (1-p)^{\beta-1}}{B(\alpha,\beta)}, \qquad \alpha>0, \beta>0$$


5. Heterogeneity in $\theta$ follows a beta distribution with parameters $\gamma$ and $\delta$. 
$$ f(\theta \; | \; \gamma,\delta) = \frac{\theta^{\gamma-1} (1-\theta)^{\delta-1}}{B(\gamma,\delta)}, \qquad \gamma>0, \delta>0$$

6. The purchase probability $p$ and the dropout probability $\theta$ vary **independently** across customers.


## Loading the data

Here is the donation data we mentioned in the lecture.  All donors in this cohort made their first donation in 1995. What we have is their **repeat** donation history 1996-2006.  We fit the model to only the **repeat** data, not the **first donation**.

```{r out.width = '90%', fig.align = "center"}

data(donationsSummary)

# add first purchase to beginning

#donations<-c(11104,donationsSummary$annual.trans)

par(mfrow=c(1,1))
par(mai=c(.8,.8,.2,.2))
plot(seq(1996,2006,1),donationsSummary$annual.trans, type="b", ylab="Total number of repeat transactions", xlab="Year", main="", xaxt='n')
x.tickmarks.yrs.all <- c( "'96","'97","'98","'99","'00","'01","'02","'03","'04","'05","'06" )
#axis(1, at = seq(0, 11, by = 1))
axis(1, at=seq(1996,2006,1),labels = x.tickmarks.yrs.all)
abline(v=2001.5,col = "red", lwd = 2)
text(x = 1999,y = 5000,"Calibration", cex=1, pos=3, col="black", font = 2)
text(x = 2004,y = 5000,"Validation", cex=1, pos=3, col="black", font = 2)
```

Our calibration data is 1996-2001, so it lasts 6 periods ($n=6$). 
We validate the model using years 2002-2006. 
All we need to esimtate the model are the sufficient statistics:

* **"reverse" recency ($t_x$)**: the last period a donation occurred. Since are data comprise 6 periods, the most recent donation is 6, or $t_x=6$.  If no repeat donations were observed, $t_x = 0$.  (Usually marketers think of recency as the number of periods **since** last purchase, $n-t_x$. Here recency is time after first purchase.)  Note further that if $x=0, \; t_x=0$.
* **frequency ($x$)**: the number of repeat donations observed in the six subsequent periods. 
* **number of purchase opportunities ($n$)**: this is usually the same for everyone.  In this case $n=6$.

There are `r nrow(donationsSummary$rf.matrix)`  recency-frequency combinations. 
The number of customers in each cell is below.

```{r}
## Get the calibration period recency-frequency matrix from the donation data:
rf.matrix <- donationsSummary$rf.matrix
rf.matrix
```

You can see, for example, that there are `r rf.matrix[1,4]` customers who are "6 for 6".  And there are `r rf.matrix[22,4]` customers who made no repeat donations, "0 for 6".  The model is going to have to account for these differences.


---

#### Comprehension Check

> *Not every donation "path" e.g., 100101, has a unique set of sufficient statistics.  What are the possible different paths when $(x=5, t_x=6, n=6)$ ?*

> [DISCUSS HERE]

---

## Estimate parameters for the BG/BB model from the recency-frequency matrix:

The likelihood is a function of $n$ transaction opportunities; recency, the period of the last donation $t_x$, and $x$, frequency, the number of donations:
$$
L(\alpha, \beta, \gamma, \delta \mid n, t_x, x) = \underbrace{\frac{B(\alpha+x, \beta + n-x)}{B(\alpha, \beta)} \frac{B(\gamma, \delta + n)}{B(\gamma, \delta)}}_\text{alive all periods} +  \underbrace{ \sum_{i=0}^{n-t_x-1} \; \frac{B(\alpha+x, \beta + t_x -x + i )}{B(\alpha, \beta)} \frac{B(\gamma+1, \delta + t_x+i)}{B(\gamma, \delta)}}_\text{all paths with death before end}
$$

If $t_x=n$, the donor is "6 for 6". This means that he/she must be alive at the end of the last period, since only alive customers can make donations.  In that case, 
$$
L(\alpha, \beta, \gamma, \delta \mid n, t_x=n, x) = \frac{B(\alpha+x, \beta + n-x)}{B(\alpha, \beta)} \frac{B(\gamma, \delta + n)}{B(\gamma, \delta)}
$$

If $f_j$ is the number of customers in each of the $J$ recency-frequency cells in the rf.matrix as above ($f_1 =$ `r rf.matrix[1,4]`, $f_2 =$ `r rf.matrix[2,4]`, ...) the sample log likelihood is then:
$$
LL(\alpha, \beta, \gamma, \delta) = \sum_{j=1}^J f_j \log[L(\alpha, \beta, \gamma, \delta \mid n, t_x, x)]
$$

We give initial guesses to the four parameters in par.start.  bgbb.EstimateParameters estimates the parameters.

```{r}
#            alpha beta gamma delta
par.start <- c(1, .5, 1, .5)

params <- bgbb.EstimateParameters(rf.matrix, par.start)
params

## Check log-likelihood of the params:
LL<-bgbb.rf.matrix.LL(params, rf.matrix)

## store parameters next to names
names(params) <- c("alpha", "beta", "gamma", "delta");

round(params,2)
```

The maximized log likelihood is `r round(LL)`.


---

#### Comprehension Check

> *It's always good to check the stability of the parameter estimates. Try different starting values and see whether you get the same parameter estimates.*

> [DISCUSS HERE]

---

## Parameter Estimates and Distributions

We plot the beta distributions implied by the maximum likelihood estimates.

```{r, out.width = '100%'}
par(mfrow=c(1,2))
par(mai=c(.8,.8,.5,.2))
temp<-bgbb.PlotTransactionRateHeterogeneity(params)
par(mai=c(.8,.8,.5,.2))
temp<-bgbb.PlotDropoutRateHeterogeneity(params)
```

Remember, if $X \sim \textrm{Beta}(a,b), \; E[X] = \frac{a}{a+b}$.  So the mean of the transaction rate while alive is `r round(params[1]/sum(params[1:2]),2)` and the mean of the drop out process is `r round(params[3]/sum(params[3:4]),2)`.  


---

#### Comprehension Check

> *What do the distributions say about the buying and dropout rates? It may be useful to consult slide 29 in Lecture 6*

> [DISCUSS HERE]

---


## Model fit: aggregate

we can see how well the model does in predicting the aggregate number of donations over years.  This uses equation 8 in the FHS (2010).
```{r out.width = '90%', fig.align = "center"}
inc.annual.trans <- donationsSummary$annual.trans   # incremental annual transactions

par(mfrow=c(1,1))
## Plot the comparison of actual and expected total incremental transactions across
## both the calibration and holdout periods:
par(mai=c(.8,.8,.3,.2))

pred<-bgbb.PlotTrackingInc(params, rf.matrix, inc.annual.trans, xticklab=x.tickmarks.yrs.all)[2,]

text(x = 4,y = 5000,"Calibration", cex=1, pos=3, col="black", font = 2)
text(x = 7,y = 5000,"Validation", cex=1, pos=3, col="black", font = 2)

# checking from paper (not needed)

al<-params[1]
be<-params[2]
ga<-params[3]
de<-params[4]

nn<-seq(1,11)
N<-sum(rf.matrix[,4])
Eq8<-(al/(al+be))*(de/(ga-1))*(1-(gamma(ga+de)*gamma(1+de+nn)/(gamma(ga+de+nn)*gamma(1+de))))

cum_donations<-N*Eq8
donations<-c(cum_donations[1],diff(cum_donations))

```

## Model Fit: Conditional Expectations

A very important test of a model is how well it predicts **at the individual level, after conditioning on a particular individual history**.  Given a customer with history $(x, t_x, n)$, (a) how many purchases do we predict in the next $n^*$ periods and (b) how well does it track actual holdout purchases?

We first do (a).  Using equation 13, we can calculate the expected number of purchases for each $(x, t_x, n)$ group in the next $n^* = 5$ periods.  

```{r, out.width = '90%', fig.align = "center"}
par(mai=c(.8,.8,.5,.2))

comp<-bgbb.HeatmapHoldoutExpectedTrans(params, n.cal=6, n.star=5)
comp1<-comp
# rotate matrix so it's the same direction as the heatmap, this is just to make the numbers easier to read
rotate <- function(x) t(apply(x, 2, rev))
library(kableExtra)
kable(rotate(rotate(rotate(t(round(comp,2))))), format = "pipe")
```

A donor who donated every year except the last $(x=5, t_x=5, n=6)$ is predicted to make `r round(comp[[6,6]],2)` in the next $n^*=5$ periods.  Yet a donor with better recency but lower frequency $(x=4,t_x=6, n=6)$ has a higher expected transaction rate, `r round(comp[[5,7]],2)`. As mentioned in Fader, Hardie and Shang (2010), this highlights the importance of recency.

Now we do (b).  The actual number of total holdout purchases by customers in each RF category is given in the variable x.star.  Therefore the average number of holdout purchases per RF category is the total divided by the number of customers.  We add this to the RF matrix and reshape.
```{r}
n.star <- 5                        # Number of transaction opportunities in the holdout period
x.star <- donationsSummary$x.star  # Transactions made by each calibration period bin in the holdout period

X<-x.star/rf.matrix[,"custs"]

hol_rf_trans<-as.data.frame(cbind(rf.matrix[,1:2],X))

actual_rf<-reshape(hol_rf_trans, idvar="t.x", timevar="x", direction="wide")

# change NA's to 0
actual_rf[is.na(actual_rf)] <- 0

# re-order columns and rows
actual_rf<-actual_rf[order(actual_rf[,1]),order(actual_rf[1,])]

# make tx to rowname
rownames(actual_rf) <- actual_rf[,8]

# delete tx
actual_rf<-actual_rf[,c(-8)]

#actual_rf

# to make it look nice and rotated in same way as heatmap
kable(rotate(rotate(rotate((round(actual_rf,2))))), format = "pipe")
```

### Conditioning on frequency & recency separately

Next we can how well the model does if we condition on the frequency of transactions in the calibration period, averaging over recency.  In other words, we take everyone who has had a frequency of $x$ transactions in the calibration period, and we can compare how many actual transactions they had in the validation period with the predictions.

```{r out.width = '90%', fig.align = "center"}
par(mai=c(.8,.8,.5,.2))

## Plot the comparison of actual and conditional expected holdout period frequencies,
## binned according to calibration period frequencies:
comp<-bgbb.PlotFreqVsConditionalExpectedFrequency(params, n.star, rf.matrix, x.star)

rownames(comp) <- c("act", "exp", "bin")
comp
```

Model predictions closely track actual donations.
Donors who made zero donations 1996-2001, made on average `r round(comp[[1]],2)` donations in 2002-2006. The BG/BB model predicts slightly fewer, `r round(comp[[2]],2)`.  Donors who made a donation every year, "6 for 6", made `r round(comp[[1,7]],2)` donations in the subsequent 5 years. The model predictions are modestly higher, at `r round(comp[[2,7]],2)`. It's interesting to note that a naive prediction of a donor who is "6 for 6" so would therefore be a "5 for 5" donor in the validation period would overestimate donations by quite a lot. 

Instead of grouping customers by frequency, we can also condition on their recency, i.e., the last period they made a donation:
```{r out.width = '90%', fig.align = "center"}
par(mai=c(.8,.8,.5,.2))

comp<-bgbb.PlotRecVsConditionalExpectedFrequency(params, n.star, rf.matrix, x.star)
rownames(comp) <- c("act", "exp", "bin")
comp
```

There were `r comp[[3,7]]` donors with maximum recency, i.e., making a donation in 2001. Those donors made on average `r round(comp[[1,7]],1)` donations in the subsequent 5 years, and the model predicts that they would make `r round(comp[[2,7]],1)`.  There is a steep falloff as recency diminishes, moving from right to left in the graph that is captured by the model.

## P(Alive) & Increasing Frequency Paradox

The probability that a customer with purchase history $x, t_x n$ will be alive at the beginning of period $n + 1$ is the term in the likelihood where the customer is alive until the end divided by all the paths:
$$
P(\textrm{Alive at} \; n+1 | \; n, x, t_x) = \frac{\frac{B(\alpha+x, \beta + n-x)}{B(\alpha, \beta)} \frac{B(\gamma, \delta + n+1)}{B(\gamma, \delta)}}{L(\alpha, \beta, \gamma, \delta \mid n, t_x, x)}
$$

Let's imagine a customer who has made his or her last donation on period $t_x = 4$, but let's vary how many purchases she makes.  At most she can make 4, and at least 1.  We can ask what the model predicts is the number of purchases expected in the subsequent $n^*=5$ periods, a calculation we already did above:

```{r, out.width = '90%', fig.align = "center"}
par(mfrow=c(1,1),mai=c(.8,.8,.5,.2))
plot(comp1[2:5,5], ylab ="Expected transactions in next 5 periods", xlab="Frequency holding last donation at 4", type="b", xaxt="n")
xtick<-seq(1, 4, by=1)
axis(side=1, at=xtick, labels = TRUE)

```


What's interesting about this curve is that customer with the largest frequency is **not** the one with the highest future predicted purchases.  This is something known as the **increasing frequency paradox**.  Why?  The likelihood that he or she is still alive decreases in $x$.  

```{r, out.width = '90%', fig.align = "center"}
par(mfrow=c(1,1))
par(mai=c(.8,.8,.5,.2))

plot(bgbb.PAlive(params, x=1:4, t.x=4, n.cal=6), ylab ="Probability that customer is alive next period", xlab="Frequency holding last donation at 4", ylim=c(0,1), type="b", xaxt="n")
xtick<-seq(1, 4, by=1)
axis(side=1, at=xtick, labels = TRUE)
```

On the one hand, higher $x$ means a higher $p$ which means more expected transactions in the future.  On the other hand, if the last two periods were no purchases, a higher $x$ means that $P(alive)$ is lower.  This second effect is stronger than the first effect, resulting in a lower expectations when $x=4$ compared to when $x=2,3$.

## CLV

Given model assumptions 2 and 3, we know that the probability of making a purchase is equal to the probability that a customer is **alive** times the probability of making a purchase conditional on being alive:
$$
P(\, Y(t) = 1 \mid p, \theta) = p \, (1-\theta)^t
$$
We integrate $p$ and $\theta$ over their mixing distributions to get the proability for a randomly chosen customer:
$$
\begin{array}{ccl}
P(\, Y(t) = 1 \mid \alpha, \beta, \gamma, \delta) &=& \displaystyle \int_0^1 \int_0^1 P(\, Y(t) = 1 \mid p, \theta) \, f(p \; | \; \alpha,\beta) \, f(\theta \; | \; \gamma,\delta) \, dp \, d\theta \\
&=& \displaystyle  \left(\frac{\alpha}{\alpha+\beta}\right) \frac{B(\gamma, \delta+t)}{B(\gamma, \delta)}
\end{array}
$$

CLV is then the discounted sum of the probability of making a transaction times some average amount per transaction ($m$):

$$
\begin{array}{ccl}
E[CLV] & =  & m \; \left( 1 + \sum_{t=1}^{\infty} P(\, Y(t) = 1 \mid \alpha, \beta, \gamma, \delta) \frac{1}{(1+d)^t} \right) \\
& = & m  \; \times \textrm{DET}
\end{array}
$$

For implementing this in R, we have to choose some upper bound to the sum like $T=200$. 

```{r}
BGBBCLV<-function(params,m,d,T) {
params<-unname(params)
al<-params[1]
be<-params[2]
ga<-params[3]
de<-params[4]
CLV<-1   # at time zero there has to be a purchase
for (i in 1:T) {
    CLV<-CLV+(al/(al+be))*(beta(ga,de+i)/beta(ga,de))*1/(1+d)^{i}
}
CLV=m*CLV  # convert discount expected purchases into expected value
return(CLV)    #return the CLV
} 
  
CLV<-BGBBCLV(params = params, m=50,d=.1,T=200)
```

CLV for a random customer with parameters as esimated, $m=€50, d=.1, T=200$ is €`r round(CLV)`.

## RLV

Lastly we can calculate the residual lifetime value of a donor with history $(x,t_x,n)$. The residual lifetime value is the present value of the expected future transaction stream standing at time $t$. 
$$
\begin{array}{ccl}
E[RLV] & = & \displaystyle m \; \left ( P(\textrm{alive at} \, n) \; \sum_{t=n+1}^{\infty} \; P(Y_t = 1 \mid \textrm{alive at} \, t) \frac{P(\textrm{alive at} \, t \mid t>n)}{(1+d)^{t-n}} \right)\\
& = & \displaystyle m \times \textrm{DERT}
\end{array}
$$
DERT means Discounted expected residual transactions. The expression is complicated and can be found in Fader Hardie and Shang (2010, equation 14).

```{r fig.align = "center", out.width='90%'}
m<-50
DERT<-bgbb.rf.matrix.DERT(params, donationsSummary$rf.matrix, d=0.1)
RLV<-m*DERT
RLVmatrix<-cbind(donationsSummary$rf.matrix,round(RLV,2)) 
RLVmatrix
```

If we assume $m=50$ is the value of a donation and we use a yearly discount rate of $d=0.1$, the RLV of a customer who makes "6 for 6" repeat donations is  €`r round(RLVmatrix[1,5],2)`.  

# Pareto/NBD

This is the model that started it all off.  The basic principles are the same.  As with the BG/BB there is a transaction process, a dropout process, and heterogeneity for both processes.  For the set of assumptions, likelihood and other equations related to the model, see the original paper mentioned below in the footnotes.  However, instead of discrete opportunities to make transactions and discrete opportunities to drop out, in the Pareto/NBD both processes are in continuous time.  As a result they are rates, rather than probabilities. 

## Data loading and sufficient statistics

As with the BG/BB, we only need sufficient statistics for each customer. Only three pieces of information for every person: how many transactions they made in the calibration period (frequency), the time of their last transaction (recency), and the total time for which they were observed.  

We'll use the CDNow dataset^[Provided with the BTYD package and available at brucehardie.com. For more details, see
the documentation of the cdnowSummary data included in the package by typing ?cdnowSummary.] Data representing the purchasing behavior of 2357 CDNOW customers between January 1997 and June 1998, summarized as a customer-by-time matrix and a vector of cumulative weekly transactions.

```{r}
data(cdnowSummary)

## Get the calibration period customer-by-sufficient-statistic matrix from the cdnow data:
cbs <- cdnowSummary$cbs

head(cbs[,1:3])
```

## Estimation

There are four parameters: two for the transaction process and two for the dropout process. We estimate them using maximum likelihood:

```{r}
## Estimate parameters for the Pareto/NBD model from the CBS:
par.start <- c(0.5, 1, 0.5, 1)
params <- pnbd.EstimateParameters(cbs, par.start)
params

## Check log-likelihood of the params:
pnbd.cbs.LL(params, cbs)
```

## Distribution

The distribution implies that there are more customers with both lower rates of purchase and dropout.  The average rates imply that a purchase and dropout are about once every 1/.05 = 20 weeks.

```{r fig.align = "center", out.width='100%'}
par(mfrow=c(1,2))
par(mai=c(.8,.8,.5,.2))
temp<-pnbd.PlotTransactionRateHeterogeneity(params)
par(mai=c(.8,.8,.5,.2))
temp<-pnbd.PlotDropoutRateHeterogeneity(params)
```

## Model fit

```{r fig.align = "center", out.width='90%'}
## Plot the comparison of actual and expected calibration period frequencies:
par(mfrow=c(1,1))
par(mai=c(.8,.8,.5,.2))
pnbd.PlotFrequencyInCalibration(params, cbs, censor=7, plotZero=TRUE)

T.star <- 39                # Length of holdout period
x.star <- cbs[,"x.star"]    # Transactions made by each customer in the holdout period

## Plot the comparison of actual and conditional expected holdout period frequencies,
## binned according to calibration period frequencies:
par(mai=c(.8,.8,.5,.2))
pnbd.PlotFreqVsConditionalExpectedFrequency(params, T.star, cbs, x.star, censor=7)

## Plot the comparison of actual and conditional expected holdout period frequencies,
## binned according to calibration period recencies:
par(mai=c(.8,.8,.5,.2))
pnbd.PlotRecVsConditionalExpectedFrequency(params, cbs, T.star, x.star)
```

# BG/NBD

The Pareto/NBD can be difficult to estimate. The BG/NBD model^[Fader, P. S., Hardie, B. G., & Lee, K. L. (2005). “Counting your customers” the easy way: An alternative to the Pareto/NBD model. Marketing science, 24(2), 275-284. http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf] represents a small change in the story but is easier to implement.  The BG/NBD model, like the Pareto/NBD model, is used for non-contractual situations in which customers can make purchases at any time. It describes the rate at which customers make purchases and the rate at which they drop out with four parameters—allowing for heterogeneity in both. The key difference is that in the BG/NBD the dropout process is discrete, taking place after any transaction, rather than in continuous time.  (See the paper for details.)

```{r fig.align = "center", out.width='100%'}
data(cdnowSummary)

## Get the calibration period customer-by-sufficient-statistic matrix from the cdnow data:
cbs <- cdnowSummary$cbs

## Estimate parameters for the BG/NBD model from the CBS:
par.start <- c(1,3,1,3)
params <- bgnbd.EstimateParameters(cbs, par.start)
params

par(mfrow=c(1,2))
par(mai=c(.8,.8,.5,.2))
temp<-bgnbd.PlotTransactionRateHeterogeneity(params)
par(mai=c(.8,.8,.5,.2))
temp<-bgnbd.PlotDropoutRateHeterogeneity(params) 

## Check log-likelihood of the params:
bgnbd.cbs.LL(params, cbs)


## Plot the comparison of actual and expected calibration period frequencies:
par(mfrow=c(1,1))
bgnbd.PlotFrequencyInCalibration(params, cbs, censor=7, plotZero=TRUE)

T.star <- 39                # Length of holdout period
x.star <- cbs[,"x.star"]    # Transactions made by each customer in the holdout period

## Plot the comparison of actual and conditional expected holdout period frequencies,
## binned according to calibration period frequencies:
bgnbd.PlotFreqVsConditionalExpectedFrequency(params, T.star, cbs, x.star, censor=7)

## Plot the comparison of actual and conditional expected holdout period frequencies,
## binned according to calibration period recencies:
bgnbd.PlotRecVsConditionalExpectedFrequency(params, cbs, T.star, x.star)
```

For more extensions, I recommend looking at `CLVTools`[https://github.com/bachmannpatrick/CLVTools]
